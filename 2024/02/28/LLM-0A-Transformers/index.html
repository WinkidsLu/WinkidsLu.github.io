<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM(0A):Transformers | 𝑨𝒎𝒐𝒓 𝒗𝒊𝒏𝒄𝒊𝒕 𝒐𝒎𝒏𝒊𝒂</title><meta name="author" content="Wang Lu"><meta name="copyright" content="Wang Lu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer 模型 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM(0A):Transformers">
<meta property="og:url" content="http://example.com/2024/02/28/LLM-0A-Transformers/index.html">
<meta property="og:site_name" content="𝑨𝒎𝒐𝒓 𝒗𝒊𝒏𝒄𝒊𝒕 𝒐𝒎𝒏𝒊𝒂">
<meta property="og:description" content="Transformer 模型 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg">
<meta property="article:published_time" content="2024-02-28T12:40:55.000Z">
<meta property="article:modified_time" content="2024-03-06T02:27:57.416Z">
<meta property="article:author" content="Wang Lu">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/02/28/LLM-0A-Transformers/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM(0A):Transformers',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-06 10:27:57'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avator.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="𝑨𝒎𝒐𝒓 𝒗𝒊𝒏𝒄𝒊𝒕 𝒐𝒎𝒏𝒊𝒂"><span class="site-name">𝑨𝒎𝒐𝒓 𝒗𝒊𝒏𝒄𝒊𝒕 𝒐𝒎𝒏𝒊𝒂</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM(0A):Transformers</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-28T12:40:55.000Z" title="发表于 2024-02-28 20:40:55">2024-02-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-06T02:27:57.416Z" title="更新于 2024-03-06 10:27:57">2024-03-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h1><blockquote>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</p>
</blockquote>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/transformer.png style="zoom:50%;"/>
</div>

<ul>
<li>Transformer：一种类似seq2seq结构的模型，输入seq输出seq，输出维度由模型自身决定</li>
<li>组成部分：encoder block，decoder block，multi-head self-attention，Add&amp;Norm，前馈全连接层（Feed Forward）</li>
</ul>
<h2 id="1-Positional-Encoding"><a href="#1-Positional-Encoding" class="headerlink" title="1. Positional Encoding"></a>1. Positional Encoding</h2><ol>
<li>Transformer中直接采用正弦函数和余弦函数来编码token的位置信息（<strong>绝对位置编码</strong>）。偶数位采用sin函数，奇数位采用cos<script type="math/tex; mode=display">
PE(pos, 2i) = \sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">
PE(pos, 2i+1) = \cos(pos/10000^{2i/d_{model}})</script></li>
<li><u>三角函数的优势：对任意位置 $PE(pos+k)$ 都可以表示为 $PE(pos)$ 的线性函数，方便计算；而周期性函数不受长度限制，也能增强模型的泛化能力。</u><script type="math/tex; mode=display">
\sin(\alpha+\beta)=\sin(\alpha)\cos(\beta)+\cos(\alpha)\sin(\beta)</script><script type="math/tex; mode=display">
\cos(\alpha+\beta)=\cos(\alpha)\cos(\beta)-\sin(\alpha)\sin(\beta)</script></li>
<li>编码后进入encoder的输入：input embedding + positional encoding</li>
</ol>
<h2 id="2-Encoder-Block"><a href="#2-Encoder-Block" class="headerlink" title="2. Encoder Block"></a>2. Encoder Block</h2><ol>
<li>经典transformer的Encoder模块包括<strong>6个层层堆叠的encoder block</strong>，每个block包括</li>
</ol>
<ul>
<li>多头自注意力层</li>
<li>前馈全连接层</li>
<li>Add &amp; Norm</li>
</ul>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402270930180.png style="zoom:30%;"/>
</div>

<ol>
<li>encoder block中的数据处理过程：输出含有位置编码的embedding $\rightarrow$ multi-head self attention $\rightarrow$ residual connection残差连接（$x+f(x)$作为后面的输入） $\rightarrow$ LayerNorm</li>
</ol>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402270935303.png style="zoom:30%;"/>
</div>

<h3 id="2-1-Multi-head-self-attention"><a href="#2-1-Multi-head-self-attention" class="headerlink" title="2.1 Multi-head self-attention"></a>2.1 Multi-head self-attention</h3><p>一种自身与自身进行关联的attention机制，从而得到更好的representation来表达自身</p>
<ol>
<li>输入一系列vector，输出一系列vector，multi-head self-attention（简称MHSA）让每个vector都携带整个序列的信息</li>
<li>SA的核心在于<strong>scaled doc-production</strong>，公式为 </li>
</ol>
<script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><blockquote>
<p><em>Q: 归一化softmax的作用，为什么要scaled？</em><br><strong>归一化概述</strong></p>
<ol>
<li>训练上的意义：随着embedding维度 <script type="math/tex">d_k</script> 增大，<script type="math/tex">Q\cdot K^T</script> 也会增大，训练时容易将softmax函数推入梯度很小的区域，可能出现梯度消失，造成模型收敛困难</li>
<li>数学上的意义：假设q和k服从标准正态分布，点积后均值为0，方差为 <script type="math/tex">d_k</script>，为了抵消方差放大的影响，使结果仍然符合标准正态分布</li>
</ol>
<p><strong>softmax的梯度变化</strong></p>
<ol>
<li>输入元素的数量级对softmax最终的分布影响非常大，当输入数量级较大时，softmax函数几乎将全部的概率分布都分给了最大分量对应的标签</li>
<li>softmax在反向传播中如何求导？<strong>[面试]</strong> 假设 <script type="math/tex">X=[x_1,x_2,...,x_n]</script>，<script type="math/tex">Y=softmax(X)</script>，则 <script type="math/tex">y_i=\frac{e^{x_i}}{\sum_{j=1}^{n}e_{x_j}}</script><br>(1) 当 <script type="math/tex">i=j</script> 时<br><div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291614376.png style="zoom:50%;"/>
</div><br>(2) 当 <script type="math/tex">i\neq j</script> 时<br><div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291627776.png style="zoom:50%;"/>
</div><br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291629887.png" alt=""><br><strong>softmax出现梯度消失的原因</strong><br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291634906.png" alt=""><br>当输入x分量较大，softmax会将大部分概率分配给最大的元素，假设最大元素为x1，则softmax输出分布将产生一个接近一个one-hot的结果：y=[1,0,0,0,0,…,0]，此时矩阵变成了：<br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291636987.png" alt=""><br>综上，所有梯度都消失为0或无限接近于0，参数无法更新，模型收敛困难<br><strong>维度和点积大小的关系</strong><br>原论文中点积的解释：<br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291651410.png" alt=""><br>点积的期望方差推导：<br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291655096.png" alt=""><script type="math/tex; mode=display">D(\sum_{i}^{}Z_i)=\sum_{i}^{}D(Z_i)</script>综上，点积的期望为0，方差为dk，所以<strong>方差越大，对应点积越大</strong>，这样softmax输出分布会更偏向最大值所在的分量，一个技巧是<strong>点除以sqrt(dk)</strong>，将方差拉回1: <script type="math/tex; mode=display">D(\frac{q\cdot k}{\sqrt{d_k}})=\frac{d_k}{(\sqrt{d_k})^2}=1</script>最终将方差控制在1，也就有效控制了点积的发散，即控制了梯度消失的问题</li>
</ol>
</blockquote>
<p><u>QKV的维度：Q(6,512), K^T(512,6), V(6,512)</u></p>
<p>V的意义：保证输入输出的维度相同、带入原始输入的信息，避免信息丢失</p>
<ol>
<li><p>计算过程</p>
<ol>
<li><p>引入三个参数矩阵 $W_q$, $W_k$, $W_v$，矩阵参数是学习的主要目标。$a_1$,$a_2$,$a_3$,$a_4$ 分别与 $W_q$, $W_k$, $W_v$ 相乘得到 $(q_1,k_1,v_1)$，$(q_2,k_2,v_2)$，$(q_3,k_3,v_3)$，$(q_4,k_4,v_4)$</p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/docproduct.png style="zoom:50%;"/>
</div>
</li>
<li><p>对 <script type="math/tex">a_1</script>，使用 <script type="math/tex">q_1</script> 分别与 <script type="math/tex">(k_1,k_2,k_3,k_4)</script> 做doc-production得到attention score: <script type="math/tex">(\hat{\alpha_{1,1}},\hat{\alpha_{1,2}},\hat{\alpha_{1,3}},\hat{\alpha_{1,4}})</script></p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/kv.png style="zoom:30%;"/>
</div>

<p>对得到的 <script type="math/tex">\alpha</script> 进行scaled处理后归一化softmax处理，得到 <script type="math/tex">(\alpha_{1,1},\alpha_{1,2},\alpha_{1,3},\alpha_{1,4})</script></p>
</li>
<li><p>执行 <script type="math/tex">b^1=\sum_{i}^{}\alpha_i \cdot v_i</script>，得到最终的输出 <script type="math/tex">b_1,b_2,b_3,b_4</script>，此时每个token的vector都携带整个序列的信息</p>
</li>
<li><p>对 <script type="math/tex">a_2</script>，计算类似，处理哪个token就用谁的q分别与所有k做doc-production，将得到的结果scaled后再softmax，最终与所有v相乘求和</p>
</li>
<li><p>总结：SA本质上就是一系列的矩阵运算，可学习参数为<strong>三个矩阵参数</strong></p>
</li>
</ol>
</li>
<li><p>MHSA是SA的进阶版本，<strong>注意力头数是个根据任务调节的hyperparameter</strong></p>
<ol>
<li><p>多头的通俗理解：SA是用q去找相关的k，但是“相关”有多种不同的形式或者定义，因此需要有不同的q。</p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/mhselfatten.png style="zoom:50%;"/>
</div>
</li>
<li><p>多头的实现：a乘上Wq得到q，q再乘上一个不一样的矩阵，如图一个头以及另外一个头的例子（1一起做，2一起做）。得到多个 $b_{i,j}$，拼接后通过一个矩阵相乘做transform得到最终的输出$b$</p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/1head.png style="zoom:50%;"/>
</div>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/2.png style="zoom:50%;"/>
</div>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402252227451.png style="zoom:50%;"/>
</div>
</li>
<li><p>实际的多头，是在最后一个维度 $d_{model}$ 上切分维度，例如模型维度为512，8个头则每个头的维度为64，最后将8个头的维度拼起来</p>
</li>
</ol>
</li>
</ol>
<blockquote>
<p><em>Q: Self-attention是什么？为什么能发挥如此大的作用？计算时如果不使用(Q,K,V)三元组而仅仅使用(Q,V)或者(K,V)或者(V)行不行？</em><br>核心：SA的机制与原理，为什么使用QKV三元组<br><strong>SA机制原理</strong></p>
<ol>
<li>一种自身与自身关联的attention机制，得到更好的representation表达自身</li>
<li>attention的一种特殊机制，<script type="math/tex">Q=K=V</script>，普通的attention中K和V容易相等，序列中每个单词都会和该序列中其他所有单词进行attention规则计算</li>
<li>计算特点：跨越一句话中不同距离token，可以远距离学习到序列知识依赖和语序结构</li>
</ol>
<p><strong>多头自注意力机制原因</strong></p>
<ol>
<li>形成多个子空间，让模型关注不同方面信息，最后将各个方面信息综合</li>
<li>多头计算再综合的方式，类似CNN多个卷积核作用，不同卷积核提取不同特征，关注不同部分</li>
<li>有利于神经网络捕捉更丰富的特征信息</li>
</ol>
<p><em>Q: Transformer比RNN和LSTM的优势？</em><br><strong>并行计算能力</strong></p>
<ol>
<li>rnn任意适合输入是当前输入加上上一时刻的隐状态，意味着历史信息通过时间步逐步向后传递，后面的信息只能等到前面的计算结束，这种链式序列依赖关系无法并行运算</li>
<li>Transformer可以一次性计算单词之间的注意力分数，计算可以同步进行，实现并行</li>
</ol>
<p><strong>特征抽取能力</strong></p>
<ol>
<li>多头SA具有强大特征抽取能力</li>
</ol>
<p><em>Q: transformer为什么优于seq2seq？</em><br><strong>seq2seq两大缺陷</strong></p>
<ol>
<li>它将encoder端信息压缩成固定长度的语义向量，代表编码器端全部信息，容易造成信息损耗，也无法让decoder端解码时关注重要信息</li>
<li>无法并行，本质上和rnn以及lstm一样</li>
</ol>
<p><strong>Transformer的改进</strong><br>既可以并行，又用多头自注意力解决了encoder固定编码问题，让decoder解码时关注重要信息</p>
</blockquote>
<h3 id="2-2-Add-amp-Norm"><a href="#2-2-Add-amp-Norm" class="headerlink" title="2.2 Add &amp; Norm"></a>2.2 Add &amp; Norm</h3><p>Add表示残差连接（residual connection），Norm表示LayerNorm。该处理单元的公式表示为：</p>
<script type="math/tex; mode=display">LayerNorm(x+sublayer(x))</script><p>即当前层的输出加上当前层的输入，构成后面层的输入</p>
<ol>
<li><u>残差连接：将信息传递更深，增强模型的拟合能力，增强模型的表现。</u></li>
<li>LayerNorm：随着网络层增加，通过多层计算后参数肯能出现过大过小，方差变大等现象，使学习过程出现异常，模型收敛变慢。因此对每一层计算后数值进行规范化可以提升模型的表现。</li>
</ol>
<blockquote>
<p>LayerNorm与BatchNorm：<script type="math/tex">(x-\mu)/\sigma</script></p>
<ol>
<li>本质上都是为了稳定训练，缩小方差，避免梯度消失或者梯度爆炸，方便后续学习</li>
<li>LN对样本内所有特征做归一化（横着做），BN对一个batch中的每个特征做归一化（竖着做，不同样本的相同维度特征，跨样本之间做）</li>
<li>BN消除了特征间的差异，保留了样本间的差异，适合CV任务；LN消除了样本之间的差异，保留了样本内特征间的差异，适合NLP任务</li>
<li>LN的归一化操作只在样本内部独立开展，因此实际可以完全忽略批次的存在。因此也不用考虑保存和更新的问题且训练和测试应用模式完全一致，均值和标准差随算随用。</li>
</ol>
</blockquote>
<h3 id="2-3-Feed-Forward-Network（FFN）"><a href="#2-3-Feed-Forward-Network（FFN）" class="headerlink" title="2.3 Feed Forward Network（FFN）"></a>2.3 Feed Forward Network（FFN）</h3><ol>
<li>线性变换，ReLU激活：</li>
</ol>
<script type="math/tex; mode=display">
FFN(x) = [max(0, xW_1+b_1)]W_2+b_2</script><ol>
<li>原论文中FFN的输入输出维度都是512，FFN维度设置为4倍关系的1024，维度先升高后降低</li>
</ol>
<h2 id="3-Decoder-Block"><a href="#3-Decoder-Block" class="headerlink" title="3. Decoder Block"></a>3. Decoder Block</h2><ol>
<li>经典transformer的Decoder模块包括<strong>6个层层堆叠的decoder block</strong>，每个block包括三个子层</li>
</ol>
<ul>
<li>multi-head self-attention layer：与encoder一致，但是decoder需要做额外的look-ahead mask，对当前token后面的全部token做mask处理。该层作用是<strong>拟合decoder端信息</strong></li>
</ul>
<blockquote>
<p>mask原因: 输出端是顺序产生的，每次输出都不能泄露当前token后的信息</p>
</blockquote>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271000250.png style="zoom:30%;"/>
</div>

<ul>
<li>encoder-decoder attention layer: 区别于encoder block，这里的q来源于decoder，k和v来源于encoder，encoder和decoder之间传递信息所使用的注意力机制又叫做<strong>cross attention</strong>，decoder每一层的cross attention都是使用encoder最后一层输出的k和v参与运算的（实际上有学者尝试了不同层间进行cross attention）。该层作用是<strong>拟合encoder端信息</strong></li>
</ul>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271016071.png style="zoom:30%;"/>
</div>

<ul>
<li>feed forward network: 与encoder没有区别</li>
</ul>
<h2 id="4-Training"><a href="#4-Training" class="headerlink" title="4. Training"></a>4. Training</h2><p>假设一组句子对 <script type="math/tex">(A,B)</script>, A作为问题，B作为结果。训练transformer时，A输入encoder block，B输入decoder block</p>
<h3 id="4-1-Teacher-Forcing"><a href="#4-1-Teacher-Forcing" class="headerlink" title="4.1 Teacher Forcing"></a>4.1 Teacher Forcing</h3><p>Transformer在训练阶段使用了<strong>Teacher Forcing模式</strong>，<u>Teacher Forcing模式是一种旨在提升序列模型训练稳定性、加速模型收敛，避免错误积累的技术</u>。</p>
<ol>
<li>自回归预测：基于已产生的预测结果预测下一步的输出，自回归预测在序列预测中广泛使用，例如RNN。然而自回归预测存在的弊端是容易积累错误，尤其在模型训练初期更加明显，因为decoder在初始训练阶段难以获得高质量的预测结果，一步错步步错，当错误积累时，模型不稳定且难以收敛；自回归预测只能以串行的方式展开，难以通过并行的方式开展以提高训练效率</li>
<li>Teacher Forcing：模型在<strong>训练阶段不以预测结果作为输入，而已真实结果（ground truth）作为输入</strong>。假设一组样本 <script type="math/tex">\{(x_1, x_2,...,x_n), (\tilde{y}_1, \tilde{y}_2,..., \tilde{y}_n)\}</script>，其中x代表序列，$\tilde{y}$ 代表真实序列值，<script type="math/tex">\{y_1,y_2,...,y_n\}</script> 代表模型的预测值，则对于t时刻，自回归模式的生成与teacher forcing的生成可以分别表示为：<script type="math/tex">y_t=generate(y_1,y_2,...,y_{t-1})</script>, <script type="math/tex">y_t=generate(\tilde{y}_1, \tilde{y}_2,..., \tilde{y}_{t-1})</script></li>
</ol>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402282249238.png style="zoom:50%;"/>
</div>

<ol>
<li>Teacher Forcing最大的优势在于可以在模型训练阶段矫正错误，避免积累，增强稳定性，增大收敛速度；此外，可以一次性的输入全部目标序列，可以以并行的方式一次性的输出完整的目标序列，训练效率大幅提升。</li>
<li>Teacher Forcing的问题：自回归纯粹靠自己，TF纯粹靠答案，在真实的任务中容易导致模型在训练阶段和预测阶段的输出结果差异较大，这个问题叫做<strong>曝光误差（exposure bias）</strong>，解决方式为<strong>计划采样（scheduled sampling）</strong>，即计划采样的折中方案即在每一步的预测时，以一定的概率随机选择是用模型输出还是用真值。</li>
</ol>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402282254324.png style="zoom:50%;"/>
</div>

<h3 id="4-2-其他训练技巧"><a href="#4-2-其他训练技巧" class="headerlink" title="4.2 其他训练技巧"></a>4.2 其他训练技巧</h3><ol>
<li><p>Copy Mechanism：decoder没必要每次都自己输出，还可以在输入中复制一些东西进行输出。例如：你好库洛洛，输出：库洛洛你好，很高兴认识你，这里的库洛洛不是生成的，而是从输入中复制出来的。这个技巧适用于<strong>对话系统或摘要生成</strong></p>
</li>
<li><p>Guided Attention：要求机器在attention时具有固定的方式。例如语音辨识，语音合成要求固定的由左向右方向</p>
</li>
</ol>
<h2 id="5-Predicting（Inference）"><a href="#5-Predicting（Inference）" class="headerlink" title="5. Predicting（Inference）"></a>5. Predicting（Inference）</h2><p>生成过程是<strong>Autoregresive</strong>的，一个单词一个单词的输出，开始输入[SOS]，decoder输出一个单词token，第二次输入[SOS] token，直至最后输出[EOS]，完成inference</p>
<h3 id="5-1-Beam-Search-集束搜索"><a href="#5-1-Beam-Search-集束搜索" class="headerlink" title="5.1 Beam Search 集束搜索"></a>5.1 Beam Search 集束搜索</h3><p>Beam Search讨论的是在预测环节如何构造出一个合理的输出的问题。</p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271407021.png style="zoom:30%;"/>
</div>

<p>对于考虑理想输出，存在两种极端的解决方案：穷举搜索方法（全局最优）与贪心搜索方法（局部最优）</p>
<ul>
<li>全局最优：输出序列联合概率最大的词汇组合。记输出序列 <script type="math/tex">y=y_1,y_2,...,y_m</script>，最优序列满足：<script type="math/tex">y^*=\arg\max p(y)=\arg\max p(y_1,y_2,...,y_m)=\arg\max p(y_1)p(y_2|y_1)...p(y_m|y_1,y_2,...,y_{m-1})</script>，要获得全局最优输出，需要穷举搜索，在时间t，不管t-1输出了什么，在当前步骤都要遍历 <script type="math/tex">y_1,...,y_{t-1}</script> 找到最大的概率值，这会造成计算量暴增；</li>
</ul>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402282305607.png style="zoom:=30%;"/>
</div>

<ul>
<li>贪心搜索: 在t预测阶段，以t-1阶段得到的具有最大概率的预测词汇作为输入。贪心搜索得到的结果不见得是全局最优的。</li>
</ul>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402282306806.png style="zoom:=30%;"/>
</div>

<ul>
<li>穷举搜索放眼全局，贪心搜索只看眼前，折中方法是<strong>集束搜索（Beam Search）</strong>，transformer进行序列生成时使用的这种方法。这里的“集束”一词非常形象——在第t步的词预测中，既不像穷举搜索那样用到全部词的组合，也不像贪心搜索那样只用到前面最大的那个预测词汇，而是用到上一步概率值排在前k个的词预测作为当前步骤的输入，这里的k被称为“集束宽度”（beam width）。在Transformer中，对集束宽度的设置为4。下图基于集束搜索的序列构造方法，在该例中，假设集束宽度2，输出序列长度3.</li>
</ul>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402282307814.png style="zoom:=10%;"/>
</div>

<h2 id="6-Transformer的改进"><a href="#6-Transformer的改进" class="headerlink" title="6. Transformer的改进"></a>6. Transformer的改进</h2><div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291318141.png style="zoom:=30%;"/>
</div>

<ul>
<li>BERT（RoBERT）<ol>
<li>encoder only的模型，无法作为语言模型进行生成，只能做分类；</li>
<li>bert是Masked language model，采用self-supervised training的方式，不需要人工标注的训练，直接选择文本进行训练</li>
<li>词的预测依赖于上下文的信息</li>
</ol>
</li>
<li>GPT（BLOOM）<ol>
<li>decoder only的模型，语言生成</li>
<li>GPT是Autoregressive(casual) language model，选择文本去训练，让模型去做词语接龙</li>
<li>词的预测是从左到右的单向</li>
<li>OpenAI，LLaMA</li>
</ol>
</li>
<li>BART（T5，FLAN-T5）<ol>
<li>encoder-decoder模型</li>
<li>训练目标：文本扰动Span Corruption（Masking entire spans of words）</li>
<li>双向的</li>
<li>Facebook AI</li>
</ol>
</li>
</ul>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291328475.png style="zoom:=50%;"/>
</div>

<h2 id="7-Transformer架构优化"><a href="#7-Transformer架构优化" class="headerlink" title="7. Transformer架构优化"></a>7. Transformer架构优化</h2><ol>
<li><u>出发点：提高速度、减少内存使用、增加上下文的长度</u></li>
<li>Transformer模型的计算复杂度主要依赖于多头自注意力的复杂度：$O(n^2)$，其中n代表输入序列的维度，例如输入有6个token。早期模型会限制输入的最大序列，超过最大序列会截断。如何降低计算的复杂度？</li>
</ol>
<p>改进的核心思想是做近似计算：</p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291337488.png style="zoom:=30%;"/>
</div>

<h3 id="7-1-Sparse-Approaches-稀疏方法"><a href="#7-1-Sparse-Approaches-稀疏方法" class="headerlink" title="7.1 Sparse Approaches 稀疏方法"></a>7.1 Sparse Approaches 稀疏方法</h3><p>近似计算方法，attention计算不完全但是性能下降可以接受。</p>
<ol>
<li>Sliding window attention：token不会全部两两计算attention，每个token只关注临近的几个token，计算attention。缺点：无法处理长跨度token之间的信息</li>
<li>Dilated sliding window：上面的改进，增加了window size，同时又把sliding window里面的一些值舍弃，既增加了window size，又没有增加计算量</li>
<li>Global+sliding window：为了获得全局信息，对sliding window中的某些token做全局attention计算。可以自定义哪些token做全局计算</li>
<li>BIGBIRD：集成了所有稀疏方法的优点，含有global，window，random的attention</li>
</ol>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291345097.png style="zoom:=50%;"/>
</div>

<h3 id="7-2-推理与压缩方法"><a href="#7-2-推理与压缩方法" class="headerlink" title="7.2 推理与压缩方法"></a>7.2 推理与压缩方法</h3><p>剪枝、模型蒸馏、量化。</p>
<div align="center">
<img src=https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291350997.png style="zoom:=25%;"/>
</div>

<p>以蒸馏为例，有一个teacher model和student model，teacer model能力强，训练好；再训练一个模型尽可能花费少，而又希望student model性能类似于teacher model？</p>
<ol>
<li>要训练student model的数据输入teacher model，输出了teacher model的hidden state；输入同时输入student model，得到hidden state</li>
<li>两个hidden state做distillation loss，让teacher model的hidden state指导studen model的hidden state.</li>
</ol>
<blockquote>
<p>Q: Transformer架构的并行化如何进行？具体体现在哪里？<br><strong>Transformer架构的并行化主要集中encoder模块上</strong><br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291706915.png" alt=""></p>
<ol>
<li>最底层整个序列所有token可以并行进行embedding，这一层处理<strong>无依赖关系（可并行）</strong></li>
<li>SA层计算token的注意力分数存在依赖性，必须要等到序列中所有单词完成embedding才可进行，但是从实际上看，计算注意力分布时采用的是矩阵运算，即可以一次性计算出所有token的注意力张量，从这个角度看实现了并行，只是矩阵运算的并行与词嵌入的并行在概念上不同而已</li>
<li>FFN层对不同的向量z也没有依赖关系，可以并行化处理，即所有的向量z输入FFN计算可以同步进行，互不干扰<br><strong>其次，Transformer架构的并行化也在decoder模块上</strong><br><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402291713302.png" alt=""></li>
<li>Decoder在<strong>训练阶段实现并行化</strong>，其中self-attention和cross- attention也在进行矩阵乘法，在进行embedding和feed forward处理时，由于各个token之间没有依赖关系，所以也是可以完全并行化，与encoder理解一致</li>
<li>Decoder在<strong>预测阶段不认为采用了并行化</strong>，因为第一个时间步输入只是SOS，后面每一个时间步输入只是依次添加之前所有预测的token</li>
<li>最重要的区别：假设训练阶段目标文本有20个token，在训练过程中是一次性输入给decoder，可以做到一些子层并行化；但是在预测阶段，需要重复处理20次循环过程，每次的输入添加进去一个token，每次输入比上次多一个token，所以不是并行的</li>
</ol>
</blockquote>
<p>参考资料：</p>
<blockquote>
<ol>
<li>李宏毅transformer，bert</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/644211164">https://zhuanlan.zhihu.com/p/644211164</a></li>
<li>BiliBili：学大模型的Scott</li>
</ol>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Wang Lu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/02/28/LLM-0A-Transformers/">http://example.com/2024/02/28/LLM-0A-Transformers/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">𝑨𝒎𝒐𝒓 𝒗𝒊𝒏𝒄𝒊𝒕 𝒐𝒎𝒏𝒊𝒂</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/28/LLM-0B-BERT/" title="LLM(0B):BERT、ELMo、GPT、HMM、CRF、序列模型简介"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LLM(0B):BERT、ELMo、GPT、HMM、CRF、序列模型简介</div></div></a></div><div class="next-post pull-right"><a href="/2024/02/19/LeetCode-6-%E9%93%BE%E8%A1%A8-%E5%88%A0%E9%99%A4%E7%B3%BB%E5%88%97/" title="LeetCode(6):链表-删除系列"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LeetCode(6):链表-删除系列</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/01/18/Chinese-LLaMA-Alpaca-3-API%E6%8E%A5%E5%8F%A3%E4%B8%8EPython%E8%B0%83%E7%94%A8/" title="Chinese-LLaMA-Alpaca(3):API接口与Python调用"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/llama2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-18</div><div class="title">Chinese-LLaMA-Alpaca(3):API接口与Python调用</div></div></a></div><div><a href="/2024/01/18/Chinese-LLaMA-Alpaca-1-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C/" title="Chinese-LLaMA-Alpaca(1):本地部署与运行"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/llama2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-18</div><div class="title">Chinese-LLaMA-Alpaca(1):本地部署与运行</div></div></a></div><div><a href="/2024/01/18/Chinese-LLaMA-Alpaca-2-Transformers%E6%8E%A8%E7%90%86%E4%B8%8EwebUI%E7%95%8C%E9%9D%A2/" title="Chinese-LLaMA-Alpaca(2):Transformers推理与webUI界面"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/llama2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-18</div><div class="title">Chinese-LLaMA-Alpaca(2):Transformers推理与webUI界面</div></div></a></div><div><a href="/2024/02/29/LLM-1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E5%89%8D%E6%B2%BF/" title="LLM(1):大模型导论 —— 技术前沿"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-29</div><div class="title">LLM(1):大模型导论 —— 技术前沿</div></div></a></div><div><a href="/2024/03/08/LLM-2-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E7%AE%80%E4%BB%8B/" title="LLM(2):大模型微调技术简介"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-08</div><div class="title">LLM(2):大模型微调技术简介</div></div></a></div><div><a href="/2024/02/28/LLM-0B-BERT/" title="LLM(0B):BERT、ELMo、GPT、HMM、CRF、序列模型简介"><img class="cover" src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="title">LLM(0B):BERT、ELMo、GPT、HMM、CRF、序列模型简介</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avator.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Wang Lu</div><div class="author-info__description">学习笔记 & 生活记录</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/WinkidsLu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:winkidslu@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">Transformer 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Positional-Encoding"><span class="toc-number">1.1.</span> <span class="toc-text">1. Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Encoder-Block"><span class="toc-number">1.2.</span> <span class="toc-text">2. Encoder Block</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Multi-head-self-attention"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 Multi-head self-attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Add-amp-Norm"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 Add &amp; Norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Feed-Forward-Network%EF%BC%88FFN%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 Feed Forward Network（FFN）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Decoder-Block"><span class="toc-number">1.3.</span> <span class="toc-text">3. Decoder Block</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Training"><span class="toc-number">1.4.</span> <span class="toc-text">4. Training</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Teacher-Forcing"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 Teacher Forcing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%85%B6%E4%BB%96%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 其他训练技巧</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Predicting%EF%BC%88Inference%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">5. Predicting（Inference）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Beam-Search-%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 Beam Search 集束搜索</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Transformer%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="toc-number">1.6.</span> <span class="toc-text">6. Transformer的改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Transformer%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96"><span class="toc-number">1.7.</span> <span class="toc-text">7. Transformer架构优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Sparse-Approaches-%E7%A8%80%E7%96%8F%E6%96%B9%E6%B3%95"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 Sparse Approaches 稀疏方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E6%8E%A8%E7%90%86%E4%B8%8E%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 推理与压缩方法</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/09/LLM-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF-%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AFRAG/" title="LLM(3):大模型技术-检索增强技术RAG">LLM(3):大模型技术-检索增强技术RAG</a><time datetime="2024-03-09T14:44:15.000Z" title="发表于 2024-03-09 22:44:15">2024-03-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/09/LLM-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF-Stable-Diffusion/" title="LLM(3):大模型技术-Stable Diffusion">LLM(3):大模型技术-Stable Diffusion</a><time datetime="2024-03-09T14:43:52.000Z" title="发表于 2024-03-09 22:43:52">2024-03-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/09/LLM-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B9%8BP-Tuning/" title="LLM(3):大模型微调之P-Tuning">LLM(3):大模型微调之P-Tuning</a><time datetime="2024-03-09T14:43:05.000Z" title="发表于 2024-03-09 22:43:05">2024-03-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/09/LLM-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B9%8BLoRA-QLoRA/" title="LLM(3):大模型微调之LoRA&amp;QLoRA">LLM(3):大模型微调之LoRA&amp;QLoRA</a><time datetime="2024-03-09T14:42:52.000Z" title="发表于 2024-03-09 22:42:52">2024-03-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/09/LLM-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B9%8BPrompt-Tuning/" title="LLM(3):大模型微调之Prompt Tuning"><img src="https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLM(3):大模型微调之Prompt Tuning"/></a><div class="content"><a class="title" href="/2024/03/09/LLM-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B9%8BPrompt-Tuning/" title="LLM(3):大模型微调之Prompt Tuning">LLM(3):大模型微调之Prompt Tuning</a><time datetime="2024-03-09T14:42:25.000Z" title="发表于 2024-03-09 22:42:25">2024-03-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://raw.githubusercontent.com/WinkidsLu/wordpress_image/main/202402271344208.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Wang Lu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="/js/tw_cn.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div></div></body></html>